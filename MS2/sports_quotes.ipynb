{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7856f7de-497e-485e-a96e-ecd8581f29ee",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd39dcc-ebf1-4563-b9fe-a7d4d9c32039",
   "metadata": {},
   "source": [
    "We use this file once to pre-process the entire `quotebank` dataset. The idea is to reduce the size of the data to only include quotes that might (or might not) interest us later. We will see/decide at the end whether this step was necessary/useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef200b8-9101-44c0-95a4-dae054f222e1",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42386c70-e39f-4578-8fce-85eca4451074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdff9782-b353-413e-9809-33548a7feb54",
   "metadata": {},
   "source": [
    "We will start by defining the environement (constants, create directories) that will help us later during the filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb23cd59-100f-4664-8682-fb0ead26a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE: int    = 1 << 16      # size of the chunks we process instead of the entire file at once\n",
    "DATA_DIR_PATH: str = 'quotebank'  # directory containing the initial compressed quotebank dataset\n",
    "OUT_DIR_PATH: str  = 'out'        # output directory where the program will dump the filtered files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edbf8f6-c93a-4d3c-a365-6b0f0649924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c65a25c-49fd-4e15-bc29-967b290cc3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = listdir(DATA_DIR_PATH)\n",
    "data_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb82a3-0bdb-4a74-b677-716ef74cdb2c",
   "metadata": {},
   "source": [
    "## Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa93547-7785-4a1d-bbf2-9fff82dc34a2",
   "metadata": {},
   "source": [
    "Here's the bulk of the program. The main idea is as follows:\n",
    "\n",
    "For each file of the quotebank dataset do:\n",
    "- break the file in multiple chunks for easier processing (the entire file would probably not fit in memory)\n",
    "- for each chunk do:\n",
    "  - create a new dataframe `sports_quotes` that consist of quotes containing the word \"sport\" in the url\n",
    "  - write the filtered quotes to a `.csv` file\n",
    "- print status update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53e525ae-aa9d-4b8b-ac0f-7fb516b83f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing chunk #0 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #16 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #32 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #48 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #64 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #80 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #96 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #112 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #128 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #144 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #160 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #176 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #192 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #208 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #224 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #240 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #256 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #272 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #288 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      "  - Processing chunk #304 (size = 65536) for file 'quotebank/quotes-2015.json.bz2'\n",
      ">>> Processed a total of 319 chunks for file 'quotebank/quotes-2015.json.bz2' => total of 3296787 lines out of 20905984\n",
      "  - Processing chunk #0 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #16 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #32 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #48 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #64 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #80 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #96 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #112 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #128 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #144 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #160 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #176 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #192 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      "  - Processing chunk #208 (size = 65536) for file 'quotebank/quotes-2016.json.bz2'\n",
      ">>> Processed a total of 212 chunks for file 'quotebank/quotes-2016.json.bz2' => total of 2286930 lines out of 13893632\n",
      "  - Processing chunk #0 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #16 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #32 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #48 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #64 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #80 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #96 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #112 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #128 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #144 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #160 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #176 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #192 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #208 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #224 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #240 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #256 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #272 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #288 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #304 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #320 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #336 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #352 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #368 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #384 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      "  - Processing chunk #400 (size = 65536) for file 'quotebank/quotes-2017.json.bz2'\n",
      ">>> Processed a total of 407 chunks for file 'quotebank/quotes-2017.json.bz2' => total of 4621534 lines out of 26673152\n",
      "  - Processing chunk #0 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #16 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #32 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #48 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #64 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #80 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #96 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #112 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #128 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #144 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #160 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #176 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #192 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #208 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #224 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #240 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #256 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #272 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #288 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #304 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #320 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #336 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #352 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #368 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #384 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      "  - Processing chunk #400 (size = 65536) for file 'quotebank/quotes-2018.json.bz2'\n",
      ">>> Processed a total of 416 chunks for file 'quotebank/quotes-2018.json.bz2' => total of 4274811 lines out of 27262976\n",
      "  - Processing chunk #0 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #16 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #32 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #48 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #64 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #80 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #96 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #112 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #128 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #144 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #160 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #176 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #192 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #208 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #224 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #240 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #256 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #272 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #288 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #304 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      "  - Processing chunk #320 (size = 65536) for file 'quotebank/quotes-2019.json.bz2'\n",
      ">>> Processed a total of 333 chunks for file 'quotebank/quotes-2019.json.bz2' => total of 2922983 lines out of 21823488\n",
      "  - Processing chunk #0 (size = 65536) for file 'quotebank/quotes-2020.json.bz2'\n",
      "  - Processing chunk #16 (size = 65536) for file 'quotebank/quotes-2020.json.bz2'\n",
      "  - Processing chunk #32 (size = 65536) for file 'quotebank/quotes-2020.json.bz2'\n",
      "  - Processing chunk #48 (size = 65536) for file 'quotebank/quotes-2020.json.bz2'\n",
      "  - Processing chunk #64 (size = 65536) for file 'quotebank/quotes-2020.json.bz2'\n",
      "  - Processing chunk #80 (size = 1569) for file 'quotebank/quotes-2020.json.bz2'\n",
      ">>> Processed a total of 81 chunks for file 'quotebank/quotes-2020.json.bz2' => total of 641614 lines out of 5308416\n",
      "--- 7471.038199901581 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for filename in data_files:\n",
    "    file_path: str = '{}/{}'.format(DATA_DIR_PATH, filename)\n",
    "    comp_ext:  str = file_path.split('.')[-1]\n",
    "    out_path:  str = '{}/sport-{}.csv'.format(OUT_DIR_PATH, filename.split('.')[0])\n",
    "    \n",
    "    with pd.read_json(file_path, lines=True, compression=comp_ext, chunksize=CHUNK_SIZE) as df_reader:\n",
    "        \n",
    "        i: int = 0 # keeps track of the chunk number\n",
    "        total_lines: int = 0 # keeps track of output file length\n",
    "        export_header: bool = True\n",
    "        \n",
    "        for chunk in df_reader:   \n",
    "            if (i & 15 == 0):\n",
    "                print(f\"  - Processing chunk #{i} (size = {chunk.shape[0]}) for file '{file_path}'\")\n",
    "            \n",
    "            # keep only lines containing the 'sport' substring in the url(s)\n",
    "            sports_quotes = chunk[[any('sport' in url for url in url_list) for url_list in chunk.urls]]\n",
    "\n",
    "            sports_quotes.to_csv(out_path, mode='a', header=export_header)\n",
    "            export_header = False\n",
    "\n",
    "            total_lines += sports_quotes.shape[0]\n",
    "            i += 1\n",
    "\n",
    "        # summary at the end of a file\n",
    "        print(f\">>> Processed a total of {i} chunks for file '{file_path}' => total of {total_lines} lines out of {i * CHUNK_SIZE}\")\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166748a8-29f6-4694-ae9b-cf595416d869",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c5ce4-f9ca-4ced-bb4d-58598e4080ef",
   "metadata": {},
   "source": [
    "First of all, we observe that the whole computation took a total of just above 2 hours, which is reasonable for a dataset of this size (19GB compressed). But was it useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79870a1-d824-4b21-a58f-f21f3fbbaa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124.51666666666667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7471 / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f758f5-3ed0-4b8d-a630-a92957686565",
   "metadata": {},
   "source": [
    "To answer the previous daunting question, we finish by compressing the produced `.csv` files into `.bz2` files for two reasons:\n",
    "1. so that we are working with smaller files when we later use our filtered dataset\n",
    "2. so that we can compare the filtered size with the original quotebank size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5334ed80-0698-4979-98c7-ab22aa063a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  out/sport-quotes-2015.csv:  4.201:1,  1.904 bits/byte, 76.20% saved, 2586161046 in, 615620062 out.\n",
      "  out/sport-quotes-2016.csv:  4.384:1,  1.825 bits/byte, 77.19% saved, 2052387478 in, 468111951 out.\n",
      "  out/sport-quotes-2017.csv:  4.561:1,  1.754 bits/byte, 78.07% saved, 5103764884 in, 1119072564 out.\n",
      "  out/sport-quotes-2020.csv:  4.717:1,  1.696 bits/byte, 78.80% saved, 578289196 in, 122593618 out.\n",
      "  out/sport-quotes-2019.csv:  4.671:1,  1.713 bits/byte, 78.59% saved, 2621718521 in, 561288174 out.\n",
      "  out/sport-quotes-2018.csv:  4.769:1,  1.678 bits/byte, 79.03% saved, 4473331888 in, 938035681 out.\n"
     ]
    }
   ],
   "source": [
    "!find out -iname '*.csv' -exec bzip2 -kzv {} \\;\n",
    "!mkdir out_bz2\n",
    "!find out -iname '*.csv.bz2' -exec mv {} out_bz2 \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a1b08-9974-4710-813b-5cf53f5b55f4",
   "metadata": {},
   "source": [
    "As a final step, we compare the size of the original compressed dataset (19GB) and the output compressed dataset (3.6GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1efcb7d-f1a3-4710-8816-a6f494391e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19G\tquotebank\n",
      " 16G\tout\n",
      "3.6G\tout_bz2\n"
     ]
    }
   ],
   "source": [
    "!du -h quotebank    # initial '.json.bz2' files\n",
    "!du -h out          # output '.csv' files\n",
    "!du -h out_bz2      # compressed output ('.csv.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e72e8bab-bfc0-48ce-b001-a016c1683d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18947368421052632"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.6 / 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be61f5-975e-4719-9b87-983ec3037495",
   "metadata": {},
   "source": [
    "Naively, we deduce that our filtered dataset is about 5 times smaller than the original (removed 81% of the quotes). However, this is only an estimation. Let's try to see why by finding a quote that appears in both files (original `json` file (here the one from milestone 1) and final `csv` file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b10aff-d919-4517-8ec2-f5af4a6c6841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"quoteID\": \"2019-01-13-028337\", \"quotation\": \"It's crazy. I can't even really explain it right now.\", \"speaker\": \"Todd Gurley II\", \"qids\": [\"Q7812406\"], \"date\": \"2019-01-13 15:55:44\", \"numOccurrences\": 1, \"probas\": [[\"Todd Gurley II\", \"0.5824\"], [\"None\", \"0.413\"], [\"C.J. Anderson\", \"0.0046\"]], \"urls\": [\"https://www.nytimes.com/2019/01/13/sports/rams-nfl-playoffs-cj-anderson.html\"], \"phase\": \"E\"}\n"
     ]
    }
   ],
   "source": [
    "!head -n 16 quotes-2019-nytimes.json | grep sport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca2257-3403-4761-8f02-9be6223b8672",
   "metadata": {},
   "source": [
    "We found a quote containing the word \"sport\" inside the url in the first 16 quotes. Let's try to find it in our final `csv` from 2019. We will try to find it in the first $n = 2^{10}$ lines for efficiency reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbdab444-c9aa-4358-981b-55cce65b98c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835,2019-01-13-028337,It's crazy. I can't even really explain it right now.,Todd Gurley II,['Q7812406'],2019-01-13 15:55:44,1,\"[['Todd Gurley II', '0.5824'], ['None', '0.413'], ['C.J. Anderson', '0.0046']]\",['https://www.nytimes.com/2019/01/13/sports/rams-nfl-playoffs-cj-anderson.html'],E\n"
     ]
    }
   ],
   "source": [
    "!head -n 1024 out/sport-quotes-2019.csv | grep \"2019-01-13-028337\"  # grep with the previous quoteID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb2ddf-55e6-46fd-9535-98a8c8b91f56",
   "metadata": {},
   "source": [
    "By comparing the same quote written in `json` and `csv`, we realise that the first contains 400 characters and the second only 290. This is due to the fact that the `json` quotes are stored inside `json` objects with repetitive object fields (e.g. \"quoteID\", \"speaker\"). Thus even if the program did not filter any lines, the output `csv` would still be smaller.\n",
    "\n",
    ":information_source: as a final note, we thought about about the fact that the `bz2` compression is probably able to compress `json` way more easily, thus comparing file size is definitely not the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb5385e-0c4f-4ab0-841b-831257ca3693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     400\n",
      "     290\n"
     ]
    }
   ],
   "source": [
    "!head -n 16 quotes-2019-nytimes.json | grep sport | wc -m\n",
    "!head -n 1024 out/sport-quotes-2019.csv | grep \"2019-01-13-028337\" | wc -m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ffa11-6ec2-4ea5-803b-768cf09c57ef",
   "metadata": {},
   "source": [
    "Thus the only reasonable way to estimate the efficiency of our program is by looking at the calculation output. If we take only quotes from 2019, we observe the following:\n",
    "```pseudocode\n",
    ">>> Processed a total of 333 chunks for file 'quotebank/quotes-2019.json.bz2' => total of 2922983 lines out of 21823488\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52de6857-e53f-43de-ab60-abee9636f020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1339374805713917"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2922983 / 21823488"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051f37f-0e0a-4234-a69e-1fd28cae057c",
   "metadata": {},
   "source": [
    "A good estimation is then sayinng that we removed 87% of the file (instead of 81%). We think that we achieved both goals stated in the [Preprocessing](#Preprocessing) introduction, namely:\n",
    "1. reduce the dataset to a reasonable size\n",
    "2. only keep information that might interest us later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c03a87-8307-44a5-973a-490c007282ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
